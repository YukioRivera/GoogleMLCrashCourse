Framing 
- How to frame a task as a ML prob. 
- Supervised Learning 
  - Label: It is the thing we are predicting. While training a model you are providing labels that is typically represented as "y"
    Labels can be the future price of wheat, kind of animal in picture, etc
  - features: input variables describing data. typically represented by the "x" variables (x1, x2, xn)
    simple projects can use single feature but can be millions in larger projects.
      In the spam detector example, the features could include the following:
      words in the email text
      sender's address
      time of day the email was sent
      email contains the phrase "one weird trick."
  - unlabled: has {features?}: (x,y). this is used to predict on new data 
  - model: maps examples to predicated variables - y. Defined by internal parameters which are learned

  - Examples: a particular instance of data, **x** (We put x in boldface to indicate that it is a vector.)
    Two categories od examples:
      - labeled examples: includes both features and the label. that is:
        example - labeled examples: {features, label}: (x, y)
      - use labeled examples to train the model. in spam detector example, the labeled examples would be individual emails that user have marked as spam or not spam
      For example, a table shows 5 labeled examples from a data set containing information about housing prices in California:
        so housing data wil have medianage(feature), totalRooms(feature), totalbedrooms(feature), and the result medianhousevalue(label)
      - unlabeled: example contains features with no labels - {features, ?}: (x, ?)
      For example, Here are 3 unlabeled examples from the same housing dataset, which exclude medianHouseValue:
        so housing data will have medianage(feature), totalRooms(feature), totalbedrooms(feature), with no result 

  Models: a model defines a relationship between features and label. What is the machine predicting, doing, etc. Example, spam detection model can associate certain features with spam
    - traning means creating or learning the mdoel. show model examples and gradually learn the relationship  between features and label 
    - interence means applying the trained model to unlabeled examples. that is can the model do what it was trained to do? predict, categorize image, etc. For example, during inference, you can predict medianHouseValue for new unlabeled examples.
  
  Regression vs Classification: 
    - regression: predicts continuous values. for example, regression models make predictions that answer questions like:
      - what is the value of a house in cali
      - what is the probability that a user will click this ad 
    - classification: predicts discrete values. For example, classification models make predictions that answer questions like: 
      - is a given email message spam or not spam 
      - is this image of a dog or cat? 
  
Descending into ML
  - Linear regression is a method for finding the straight line or hyperplane that best fits a set of points. 
  Learning Objectives
    Refresh your memory on line fitting.
    Relate weights and biases in machine learning to slope and offset in line fitting.
    Understand "loss" in general and squared loss in particular.

  - A convenient Loss function for regression 
    - L(2) fir a given example is also called squared error 
    = Square of the difference betweeen prediction and label 
    = (observation - prediction)^2
    = (y - y`)^2

  - when training a model we are interested in defining loss across all our dataset not just a single point, you can use a sumation function to acomplish it 

Descending into ML: Linear Regression
  - y = mx + b is similar to the ML function, y`= b + w1x1
  - y` :  is the predicted label (a desired output)
  - b : is the bias (the y-intercept), sometimes referred to as w(0)
  - w(1) :  is the weight of feature 1. Weight is the same concept as the   "slope" m in the traditional equation of a line.
  - x(1) is a feature (a known input)

  - Training a model, means learning (determining) good values for all the weights and the bias from labeled examples 
    - in supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss, this process is called empirical risk minimization 
  
  # Squared loss: a popular loss function

  The linear regression models we'll examine here use a loss function called squared loss (also known as L(2) loss). The squared loss for a single example is as follows:

  `Loss = the square of the difference between the label and the prediction`
  `Loss = (observation - prediction(x))^2`
  `Loss = (y - y')^2`

- Mean squared error is the avg squared loss per example. its how to measure the avg loss in the dataset relative to the predicted function 
  # Mean Squared Error (MSE) Formula:

  `MSE = (1/n) * Σ(observation_i - prediction(x_i))^2`

  Where:
  - `n` is the number of examples in the dataset.
  - `Σ` denotes the summation over all examples.
  - `observation_i` is the true value for the ith example.
  - `prediction(x_i)` is the predicted value for the ith example.

Reducing Lost
- To train a model you need to reduce the model's lost 
Learning Objectives:
- learn how to train a model 
- uderstand the full gradient descent and some variants including:
  - mini batch gradient descent 
  - stochastic gradient descent 
- experiment with learning rate

Reducing Lost
  How do we reduce lost?
  - hyperparameters are the configuration settings use to tune how the model is trained 
  - derivative of (y - y`)^2 with respect to the weights and biases tell us how lloss changes for a given example 
    - simple to compute and convex 
  - so we repeatedly take small steps in the direction that minimizes loss 
    - we call these gradient steps 
    - this strategy is called gradient descent 

Weight Initialization 
  - for convex problems, weights can start anywhere (say all 0s)
    - convex: think bowel shape
    - just one minumum
  - but most ML problems, for example neural nets 
    - non-convex: think of an egg crate 
    - more than one minimum 
    - strong dependency on initial values

SGD and Mini-batch gradient descent 
  - could compute gradient over entire data set on each step, but this turns out to be unncecessary 
  - compute gradient on small batches help with computing 
    - on every step, get a new random sample 
  - Stochastic Gradient Descent: one example at a time 
  - mini-batch gradient descent: batches of 10-1000 
    - lost and gradient are averaged over the batch 

Reducing Loss: an iterative approach 
  - you use an iterative approach to develop a training model
  - you start by guessing values and the model will tell you if you are getting "warmer" to the training model 
  - the model takes one or more features as input and returns one prediction 
  - you iterate through the model until you find the loss function with the lowest possible loss 
  - you keep going until the loss stops changing or changes extremly slowly
    - when you reach this point you can say that the model has converged 

Reducing Loss: Gradient Descent 
  - 
